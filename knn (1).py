# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpGgF4Bl8iOATjDbJwyVhCkl8ccnM2a_

# KNN
"""

# Import required libraries
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import pandas as pd # Import pandas to read the CSV file

# Step 1: Load dataset
# Assuming your CSV file has features in all columns except the last one, which is the target
df = pd.read_csv("/content/drive/MyDrive/students_knn (1).csv") # Load the dataset from the provided path
X = df.iloc[:, :-1] # Features (inputs) - all columns except the last one
y = df.iloc[:, -1] # Labels (outputs) - the last column

# Remove non-numeric columns that are causing the ValueError
X = X.drop(['Name', 'Roll No', 'Email'], axis=1)

# Based on the new dataset, these columns are not present and were causing the KeyError.
# Removing the one-hot encoding for these columns.
# categorical_cols = ['proto', 'saddr', 'daddr', 'category']
# X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Convert any boolean columns to integers (0 or 1)
for col in X.columns:
    if X[col].dtype == 'bool':
        X[col] = X[col].astype(int)

# Convert 'sport' and 'dport' to numeric, handling errors
# Based on the new dataset, these columns are not present. Removing the conversion.
# X['sport'] = pd.to_numeric(X['sport'], errors='coerce').fillna(0).astype(int)
# X['dport'] = pd.to_numeric(X['dport'], errors='coerce').fillna(0).astype(int)


# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2,random_state = 65
)

# Step 3: Initialize and train the KNN classifier
knn = KNeighborsClassifier(n_neighbors=35)
knn.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = knn.predict(X_test)

# Step 5: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy: {accuracy:.2f}")

# Step 6: Evaluate accuracy for both
y_train_pred = knn.predict(X_train) # Predict on training data
test_accuracy = accuracy_score(y_test, y_pred) # Corrected variable name
train_accuracy = accuracy_score(y_train, y_train_pred)


# Step 7: Print the results
print("Training Accuracy:", train_accuracy)
print("Testing Accuracy:", test_accuracy)

from google.colab import drive
drive.mount('/content/drive')

"""# Task
Implement and evaluate a KNN model on the dataset provided at "/content/drive/MyDrive/students_knn (1).csv", addressing any data type issues and reproducing the results.

## Evaluate other metrics

### Subtask:
Calculate and display additional classification metrics like precision, recall, F1-score, and a confusion matrix.

**Reasoning**:
Calculate and display additional classification metrics as requested by the subtask.
"""

from sklearn.metrics import classification_report, confusion_matrix

# Calculate and print classification report
print(classification_report(y_test, y_pred))

# Calculate and print confusion matrix
print(confusion_matrix(y_test, y_pred))

"""## Tune hyperparameters

### Subtask:
Use techniques like cross-validation and GridSearchCV to find the best value for `n_neighbors`.

**Reasoning**:
Import necessary modules for GridSearchCV and cross-validation.
"""

from sklearn.model_selection import GridSearchCV, StratifiedKFold

"""**Reasoning**:
Define the parameter grid, initialize StratifiedKFold, and initialize GridSearchCV with the KNN classifier and defined parameters, then fit the GridSearchCV to the training data and print the best parameters and score.


"""

# Create a parameter grid for n_neighbors
param_grid = {'n_neighbors': list(range(1, 31))}

# Initialize StratifiedKFold
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=kf, scoring='accuracy')

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best value of n_neighbors
print(f"Best n_neighbors: {grid_search.best_params_['n_neighbors']}")

# Print the corresponding best cross-validation score
print(f"Best cross-validation accuracy: {grid_search.best_score_:.2f}")

"""## Save the model

### Subtask:
Save the trained KNN model to a file so you can load it later without retraining.

**Reasoning**:
Import the joblib library to save the trained model.
"""

import joblib

"""**Reasoning**:
Save the trained KNN model to a file and print a confirmation message.


"""

joblib.dump(knn, 'knn_model.pkl')
print("Trained KNN model saved to knn_model.pkl")

"""## Summary:

### Data Analysis Key Findings

*   The initial KNN model evaluation showed a perfect accuracy of 1.0 for class 1 but failed to predict any instances of class 0, resulting in undefined precision for class 0. The confusion matrix indicated that all 9 instances of class 1 were correctly predicted, while the single instance of class 0 was misclassified.
*   The dataset exhibits class imbalance, with the least populated class having only 4 members, which is less than the 5 splits used in StratifiedKFold cross-validation.
*   Hyperparameter tuning using GridSearchCV with StratifiedKFold identified `n_neighbors = 1` as the best parameter for the KNN model, achieving a perfect cross-validation accuracy of 1.00.
*   The trained KNN model was successfully saved to a file named `knn_model.pkl` using the `joblib` library.

### Insights or Next Steps

*   The perfect accuracy achieved with `n_neighbors=1` and the class imbalance suggest potential overfitting or data separation issues. Further investigation into the dataset and model validation is necessary.
*   Consider implementing techniques to address the class imbalance, such as oversampling the minority class or using evaluation metrics less sensitive to imbalance, before finalizing the model.

"""